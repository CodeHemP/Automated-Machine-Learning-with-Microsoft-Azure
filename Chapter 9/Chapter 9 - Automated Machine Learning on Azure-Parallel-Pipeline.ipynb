{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace, Dataset, Datastore\n",
    "from azureml.core import Experiment, Environment, Model\n",
    "from azureml.core.compute import ComputeTarget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.runconfig import RunConfiguration, CondaDependencies, DEFAULT_CPU_IMAGE\n",
    "from azureml.pipeline.steps import PythonScriptStep, ParallelRunStep, ParallelRunConfig\n",
    "from azureml.pipeline.core import Pipeline, PublishedPipeline, PipelineData\n",
    "from azureml.pipeline.core import StepSequence\n",
    "from azureml.widgets import RunDetails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import random as r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = Workspace.from_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_name = 'compute-cluster'\n",
    "compute_target = ComputeTarget(ws, compute_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datastore = Datastore.get_default(ws)\n",
    "my_datastore_name = 'workspaceblobstore'\n",
    "my_datastore = Datastore.get(ws, my_datastore_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sepal_length_range = np.arange(4.3, 7.9, 0.1)\n",
    "sepal_width_range = np.arange(2, 4.4, 0.1)\n",
    "petal_length_range = np.arange(1, 6.9, 0.1)\n",
    "petal_width_range = np.arange(0.1, 2.5, 0.1)\n",
    "IrisList = []\n",
    "columns = ['sepal_length','sepal_width','petal_length','petal_width']\n",
    "IrisDF = pd.DataFrame(columns=columns)\n",
    "for i in range(0,1000000):\n",
    "    values = [round(r.choice(sepal_length_range),1),round(r.choice(sepal_width_range),1),round(r.choice(petal_length_range),1),round(r.choice(petal_width_range),1)]\n",
    "    iris_dictionary = pd.DataFrame(dict(zip(columns, values)),index=[0])\n",
    "    IrisList.append(iris_dictionary)\n",
    "IrisDF = IrisDF.append(IrisList,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dataset.Tabular.register_pandas_dataframe(IrisDF,datastore,'Iris Parallel Scoring')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('Scoring_Scripts', exist_ok=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile Scoring_Scripts/Iris_Parallel_Scoring.py\n",
    "from azureml.core import Run, Workspace\n",
    "from azureml.core import Dataset, Datastore, Model\n",
    "\n",
    "import os\n",
    "import joblib\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "run = Run.get_context()\n",
    "\n",
    "def init():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--model_name', dest=\"model_name\", required=True)\n",
    "    args, unknown_args = parser.parse_known_args()\n",
    "    \n",
    "    global model\n",
    "    model_path = Model.get_model_path(args.model_name)\n",
    "    model = joblib.load(model_path)\n",
    "    \n",
    "def run(input_data):\n",
    "    predictions = model.predict(input_data)  \n",
    "    predSeries = pd.Series(predictions)\n",
    "    input_data['Prediction'] = predSeries \n",
    "    print('Data written to parallel_run_step.txt')\n",
    "    return input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile Scoring_Scripts/Iris_Parallel_Output_Creation.py\n",
    "from azureml.core import Run, Workspace\n",
    "from azureml.core import Dataset, Datastore\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import argparse\n",
    "\n",
    "run = Run.get_context()\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--input_data_folder\",type=str)\n",
    "args = parser.parse_args()\n",
    "\n",
    "def main():  \n",
    "    FileName = \"parallel_run_step.txt\"\n",
    "    input_data_path = os.path.join(args.input_data_folder, FileName)  \n",
    "    result = pd.read_csv(input_data_path, delimiter=\" \", header=None)\n",
    "    \n",
    "    \n",
    "    columns = ['sepal_length','sepal_width','petal_length','petal_width', 'Prediction']\n",
    "    result.columns = columns\n",
    "\n",
    "    ws = run.experiment.workspace\n",
    "    datastore = Datastore.get_default(ws)\n",
    "    \n",
    "    output_datastore_path = 'Output_Folder'\n",
    "    os.makedirs(output_datastore_path, exist_ok=True) \n",
    "    FileName = \"Iris_Parallel_Predictions.csv\"\n",
    "    OutputPath = os.path.join(output_datastore_path, FileName)\n",
    "    result.to_csv(OutputPath, index = False, sep=',')\n",
    "    \n",
    "    datastore.upload_files(files=[OutputPath], target_path=output_datastore_path, overwrite=True)\n",
    "    os.remove(OutputPath)\n",
    "    os.rmdir(output_datastore_path)\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--input_data_folder\",type=str)\n",
    "args = parser.parse_args()\n",
    "\n",
    "def main(): \n",
    "    input_data_path = os.path.join(args.input_data_folder) \n",
    "    result = pandas.read_parquet(input_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Env = Environment.get(ws, 'AutoML Environment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parallel_run_output = PipelineData(name='parallel_predictions', datastore=datastore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parallel_environment = Env\n",
    "parallel_environment.docker.enabled = True \n",
    "parallel_environment.docker.base_image = DEFAULT_CPU_IMAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_config = RunConfiguration()\n",
    "run_config.environment = Env\n",
    "run_config.environment.docker.enabled = True\n",
    "run_config.environment.docker.base_image = DEFAULT_CPU_IMAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is for a parallel run\n",
    "parallel_run_config = ParallelRunConfig(\n",
    "    source_directory='Scoring_Scripts/',\n",
    "    entry_script=\"Iris_Parallel_Scoring.py\",\n",
    "    mini_batch_size=\"1MB\",\n",
    "    error_threshold=5,\n",
    "    output_action=\"append_row\",\n",
    "    environment=parallel_environment,\n",
    "    compute_target=compute_target,\n",
    "    run_invocation_timeout=60,\n",
    "    node_count=4,\n",
    "    logging_level=\"DEBUG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset.get_by_name(ws,'Iris Parallel Scoring')\n",
    "input_data = dataset.as_named_input('Iris_Parallel_Scoring')\n",
    "model_name = 'Iris-Multi-Classification-AutoML'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parallel_scoring_step = ParallelRunStep(\n",
    "    name=\"iris-parallel-scoring-step\",\n",
    "    parallel_run_config=parallel_run_config,\n",
    "    inputs=[input_data],\n",
    "    output=parallel_run_output,\n",
    "    arguments=['--model_name', model_name],\n",
    "    allow_reuse=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_step = PythonScriptStep(name='iris-output-step',\n",
    "                                         script_name='Iris_Parallel_Output_Creation.py',\n",
    "                                         source_directory='Scoring_Scripts',\n",
    "                                         arguments=[\"--input_data_folder\", parallel_run_output,],\n",
    "                                         inputs=[parallel_run_output], \n",
    "                                         compute_target=compute_target,\n",
    "                                         runconfig=run_config,\n",
    "                                         allow_reuse=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_sequence = StepSequence(steps=[parallel_scoring_step, output_step])\n",
    "pipeline = Pipeline(workspace=ws, steps=step_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run your pipeline\n",
    "pipeline_experiment = Experiment(ws, 'Iris-Paralell-Scoring-Pipeline-Run')\n",
    "pipeline_run = pipeline_experiment.submit(pipeline, show_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RunDetails(pipeline_run).show()\n",
    "pipeline_run.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "published_pipeline = pipeline_run.publish_pipeline(\n",
    "    name='Iris-Parallel-Scoring-Pipeline',\\\n",
    "    description='Pipeline that Scores Iris Data in Parallel', version= '1.0')\n",
    "\n",
    "published_pipeline"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
